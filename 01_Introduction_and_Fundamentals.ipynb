{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“š Notebook 01: Introduction and Fundamentals\n",
    "\n",
    "**LangChain 1.0.5+ | Mixed Level Class**\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will understand:\n",
    "1. What LangChain is and why it's useful\n",
    "2. LangChain's modular architecture\n",
    "3. Core concepts: Documents, Chains, and LCEL\n",
    "4. How to set up your development environment\n",
    "5. The difference between LangChain and traditional ML pipelines\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“– Table of Contents\n",
    "\n",
    "1. [What is LangChain?](#what-is-langchain)\n",
    "2. [LangChain Architecture](#architecture)\n",
    "3. [Environment Setup](#setup)\n",
    "4. [Core Concepts](#core-concepts)\n",
    "5. [Quick Start Example](#quick-start)\n",
    "6. [LangChain vs Traditional Pipelines](#comparison)\n",
    "7. [Summary & Next Steps](#summary)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"what-is-langchain\"></a>\n",
    "## 1. What is LangChain? ğŸ¤”\n",
    "\n",
    "### ğŸ”° BEGINNER SECTION\n",
    "\n",
    "**LangChain** is an open-source framework that makes it easy to build applications powered by Large Language Models (LLMs) like GPT-4, Claude, or Llama.\n",
    "\n",
    "### Why LangChain?\n",
    "\n",
    "Imagine you want to build a chatbot that can:\n",
    "- Answer questions about your company's PDF documents\n",
    "- Search through your database\n",
    "- Remember previous conversations\n",
    "- Call external APIs\n",
    "\n",
    "Without LangChain, you'd need to:\n",
    "1. âœï¸ Write code to load and parse PDFs\n",
    "2. âœï¸ Convert text to embeddings\n",
    "3. âœï¸ Store embeddings in a vector database\n",
    "4. âœï¸ Implement semantic search\n",
    "5. âœï¸ Format prompts for the LLM\n",
    "6. âœï¸ Handle LLM API calls\n",
    "7. âœï¸ Manage conversation memory\n",
    "\n",
    "**With LangChain:**\n",
    "- âœ… All these components are pre-built and ready to use\n",
    "- âœ… You just connect them like LEGO blocks\n",
    "- âœ… Focus on your application logic, not infrastructure\n",
    "\n",
    "### ğŸ“ INTERMEDIATE NOTE\n",
    "\n",
    "LangChain provides:\n",
    "- **Abstractions**: Unified interfaces for different LLMs, vector stores, and tools\n",
    "- **Chains**: Composable workflows using LCEL (LangChain Expression Language)\n",
    "- **Agents**: Autonomous systems that can use tools and make decisions\n",
    "- **Memory**: Conversation history and context management\n",
    "- **Callbacks**: Monitoring, logging, and debugging hooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"architecture\"></a>\n",
    "## 2. LangChain Architecture ğŸ—ï¸\n",
    "\n",
    "### ğŸ”° BEGINNER: Visual Architecture\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    YOUR APPLICATION                     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                           â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                  LANGCHAIN FRAMEWORK                    â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
    "â”‚  â”‚ Document â”‚  â”‚   Text   â”‚  â”‚ Embeddingsâ”‚  â”‚ Vector  â”‚ â”‚\n",
    "â”‚  â”‚ Loaders  â”‚â†’ â”‚ Splittersâ”‚â†’ â”‚  Models   â”‚â†’ â”‚ Stores  â”‚ â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚\n",
    "â”‚  â”‚Retrieversâ”‚  â”‚ Prompts  â”‚  â”‚   LLMs   â”‚               â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
    "â”‚  â”‚         LCEL (LangChain Expression Language)     â”‚   â”‚\n",
    "â”‚  â”‚         Connects everything with | operator      â”‚   â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                           â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚             EXTERNAL SERVICES & DATA                    â”‚\n",
    "â”‚  â€¢ OpenAI/Anthropic APIs  â€¢ Vector Databases            â”‚\n",
    "â”‚  â€¢ PDF Files              â€¢ Websites                    â”‚\n",
    "â”‚  â€¢ Databases              â€¢ APIs                        â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### ğŸ“ INTERMEDIATE: Package Structure (LangChain 1.0+)\n",
    "\n",
    "LangChain is organized into several packages:\n",
    "\n",
    "| Package | Purpose | Example Imports |\n",
    "|---------|---------|------------------|\n",
    "| **langchain-core** | Core abstractions, base classes | `from langchain_core.documents import Document` |\n",
    "| **langchain-community** | Community integrations (loaders, vector stores) | `from langchain_community.document_loaders import PyPDFLoader` |\n",
    "| **langchain-openai** | OpenAI-specific integrations | `from langchain_openai import ChatOpenAI, OpenAIEmbeddings` |\n",
    "| **langchain-text-splitters** | Text splitting utilities | `from langchain_text_splitters import RecursiveCharacterTextSplitter` |\n",
    "\n",
    "**Why this matters:** In LangChain 1.0+, you import from specific packages instead of `langchain` directly. This reduces dependencies and improves modularity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"setup\"></a>\n",
    "## 3. Environment Setup ğŸ› ï¸\n",
    "\n",
    "### ğŸ”° BEGINNER: Step-by-Step Setup\n",
    "\n",
    "Let's verify your environment is ready!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.12.9 (main, Mar 17 2025, 21:36:21) [Clang 20.1.0 ]\n",
      "âœ… Python version is compatible\n"
     ]
    }
   ],
   "source": [
    "#CELL-NO: 1\n",
    "# Step 1: Check Python version (should be 3.9+)\n",
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "# Check if Python is 3.9 or higher\n",
    "if sys.version_info >= (3, 9):\n",
    "    print(\"âœ… Python version is compatible\")\n",
    "else:\n",
    "    print(\"âŒ Please upgrade to Python 3.9 or higher\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain version: 1.2.0\n",
      "LangChain Core version: 1.2.5\n",
      "âœ… LangChain 1.0+ detected\n"
     ]
    }
   ],
   "source": [
    "#CELL-NO: 2\n",
    "# Step 2: Import LangChain and check version\n",
    "import langchain\n",
    "from langchain_core import __version__ as core_version\n",
    "\n",
    "print(f\"LangChain version: {langchain.__version__}\")\n",
    "print(f\"LangChain Core version: {core_version}\")\n",
    "\n",
    "# We're using LangChain 1.0.5+ for this course\n",
    "if langchain.__version__ >= \"1.0\":\n",
    "    print(\"âœ… LangChain 1.0+ detected\")\n",
    "else:\n",
    "    print(\"âŒ Please upgrade: pip install --upgrade langchain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… OPENAI_API_KEY found\n",
      "âœ… GOOGLE_API_KEY found\n"
     ]
    }
   ],
   "source": [
    "#CELL-NO: 3\n",
    "# Step 3: Load environment variables (API keys)\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Check if OpenAI API key is set\n",
    "# NOTE: We don't print the actual key for security!\n",
    "if os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"âœ… OPENAI_API_KEY found\")\n",
    "else:\n",
    "    print(\"âŒ OPENAI_API_KEY not found\")\n",
    "    print(\"   Create a .env file with: OPENAI_API_KEY=your-key-here\")\n",
    "\n",
    "# Check for Google API key (optional, for Google Gemini)\n",
    "if os.getenv(\"GOOGLE_API_KEY\"):\n",
    "    print(\"âœ… GOOGLE_API_KEY found\")\n",
    "else:\n",
    "    print(\"âš ï¸  GOOGLE_API_KEY not found (optional for this notebook)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ Setting Up Your .env File\n",
    "\n",
    "If you don't have a `.env` file, create one in the project root:\n",
    "\n",
    "```bash\n",
    "# .env file\n",
    "OPENAI_API_KEY=sk-proj-your-key-here\n",
    "GOOGLE_API_KEY=your-google-key-here\n",
    "```\n",
    "\n",
    "**âš ï¸ SECURITY WARNING:**\n",
    "- Never commit `.env` files to Git\n",
    "- Add `.env` to your `.gitignore` file\n",
    "- Never hardcode API keys in your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"core-concepts\"></a>\n",
    "## 4. Core Concepts ğŸ“š\n",
    "\n",
    "### 4.1 Documents ğŸ“„\n",
    "\n",
    "### ğŸ”° BEGINNER\n",
    "\n",
    "A **Document** is LangChain's way of representing a piece of text with metadata.\n",
    "\n",
    "Think of it like a note card:\n",
    "- **Front (page_content):** The actual text\n",
    "- **Back (metadata):** Information about the text (source, page number, date, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content:\n",
      "LangChain makes building LLM applications easy!\n",
      "\n",
      "Metadata:\n",
      "{'source': 'introduction.txt', 'author': 'LangChain Team', 'date': '2025-01-15'}\n",
      "\n",
      "Source: introduction.txt\n"
     ]
    }
   ],
   "source": [
    "#CELL-NO: 4\n",
    "# Creating a Document manually\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Create a simple document\n",
    "doc = Document(\n",
    "    page_content=\"LangChain makes building LLM applications easy!\",\n",
    "    metadata={\n",
    "        \"source\": \"introduction.txt\",\n",
    "        \"author\": \"LangChain Team\",\n",
    "        \"date\": \"2025-01-15\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Access the content\n",
    "print(\"Content:\")\n",
    "print(doc.page_content)\n",
    "print(\"\\nMetadata:\")\n",
    "print(doc.metadata)\n",
    "print(f\"\\nSource: {doc.metadata['source']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ INTERMEDIATE: Why Metadata Matters\n",
    "\n",
    "Metadata is crucial for:\n",
    "1. **Citation**: Showing users where information came from\n",
    "2. **Filtering**: Only search documents from specific sources\n",
    "3. **Debugging**: Tracking which documents are being retrieved\n",
    "4. **Analytics**: Understanding which sources are most useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“„ Document 1:\n",
      "   Content: Python is a high-level programming language.\n",
      "   Category: programming\n",
      "   Difficulty: beginner\n",
      "\n",
      "ğŸ“„ Document 2:\n",
      "   Content: Machine learning is a subset of artificial intelligence.\n",
      "   Category: AI\n",
      "   Difficulty: intermediate\n",
      "\n",
      "ğŸ“„ Document 3:\n",
      "   Content: RAG combines retrieval and generation for better LLM outputs.\n",
      "   Category: AI\n",
      "   Difficulty: advanced\n"
     ]
    }
   ],
   "source": [
    "#CELL-NO: 5\n",
    "# Creating multiple documents (like a mini knowledge base)\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"Python is a high-level programming language.\",\n",
    "        metadata={\"category\": \"programming\", \"difficulty\": \"beginner\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Machine learning is a subset of artificial intelligence.\",\n",
    "        metadata={\"category\": \"AI\", \"difficulty\": \"intermediate\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"RAG combines retrieval and generation for better LLM outputs.\",\n",
    "        metadata={\"category\": \"AI\", \"difficulty\": \"advanced\"}\n",
    "    )\n",
    "]\n",
    "\n",
    "# Print all documents\n",
    "for i, doc in enumerate(documents, 1):\n",
    "    print(f\"\\nğŸ“„ Document {i}:\")\n",
    "    print(f\"   Content: {doc.page_content}\")\n",
    "    print(f\"   Category: {doc.metadata['category']}\")\n",
    "    print(f\"   Difficulty: {doc.metadata['difficulty']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 LCEL (LangChain Expression Language) ğŸ”—\n",
    "\n",
    "### ğŸ”° BEGINNER\n",
    "\n",
    "**LCEL** is a way to connect different components using the pipe operator `|`.\n",
    "\n",
    "Think of it like a factory assembly line:\n",
    "```\n",
    "Input â†’ Component 1 â†’ Component 2 â†’ Component 3 â†’ Output\n",
    "```\n",
    "\n",
    "### Before LCEL (Old Way - Messy!):\n",
    "```python\n",
    "output = component3(component2(component1(input)))\n",
    "```\n",
    "\n",
    "### With LCEL (New Way - Clean!):\n",
    "```python\n",
    "chain = component1 | component2 | component3\n",
    "output = chain.invoke(input)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ INTERMEDIATE: LCEL Deep Dive\n",
    "\n",
    "LCEL provides:\n",
    "- **Streaming**: Stream outputs as they're generated\n",
    "- **Batch Processing**: Process multiple inputs efficiently\n",
    "- **Async Support**: Non-blocking operations\n",
    "- **Debugging**: Better error messages and logging\n",
    "- **Type Safety**: Better IDE autocomplete\n",
    "\n",
    "**Key Methods:**\n",
    "- `.invoke(input)`: Process single input\n",
    "- `.batch([input1, input2])`: Process multiple inputs\n",
    "- `.stream(input)`: Stream output tokens\n",
    "- `.ainvoke(input)`: Async version of invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 'hello langchain'\n",
      "\n",
      "Processing:\n",
      "  Step 1: uppercase â†’ HELLO LANGCHAIN\n",
      "  Step 2: add_prefix â†’ RESULT: HELLO LANGCHAIN\n",
      "  Step 3: add_emoji â†’ âœ… RESULT: HELLO LANGCHAIN\n",
      "\n",
      "Final Output: âœ… RESULT: HELLO LANGCHAIN\n"
     ]
    }
   ],
   "source": [
    "#CELL-NO: 6\n",
    "# Simple LCEL Example: String Transformation Chain\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# Create simple transformation functions\n",
    "def uppercase(text: str) -> str:\n",
    "    \"\"\"Convert text to uppercase\"\"\"\n",
    "    print(f\"  Step 1: uppercase â†’ {text.upper()}\")\n",
    "    return text.upper()\n",
    "\n",
    "def add_prefix(text: str) -> str:\n",
    "    \"\"\"Add a prefix to text\"\"\"\n",
    "    result = f\"RESULT: {text}\"\n",
    "    print(f\"  Step 2: add_prefix â†’ {result}\")\n",
    "    return result\n",
    "\n",
    "def add_emoji(text: str) -> str:\n",
    "    \"\"\"Add emoji to text\"\"\"\n",
    "    result = f\"âœ… {text}\"\n",
    "    print(f\"  Step 3: add_emoji â†’ {result}\")\n",
    "    return result\n",
    "\n",
    "# Create runnables (components that can be chained)\n",
    "uppercase_runnable = RunnableLambda(uppercase)\n",
    "prefix_runnable = RunnableLambda(add_prefix)\n",
    "emoji_runnable = RunnableLambda(add_emoji)\n",
    "\n",
    "# Build the chain using LCEL\n",
    "chain = uppercase_runnable | prefix_runnable | emoji_runnable\n",
    "\n",
    "# Execute the chain\n",
    "print(\"Input: 'hello langchain'\")\n",
    "print(\"\\nProcessing:\")\n",
    "result = chain.invoke(\"hello langchain\")\n",
    "print(f\"\\nFinal Output: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Chains ğŸ”—\n",
    "\n",
    "### ğŸ”° BEGINNER\n",
    "\n",
    "A **Chain** is a sequence of operations. Common chain types:\n",
    "\n",
    "1. **Simple Chain**: Input â†’ Process â†’ Output\n",
    "2. **RAG Chain**: Query â†’ Retrieve â†’ Generate â†’ Answer\n",
    "3. **Multi-step Chain**: Input â†’ Step 1 â†’ Step 2 â†’ Step 3 â†’ Output\n",
    "\n",
    "We'll build complex chains in later notebooks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"quick-start\"></a>\n",
    "## 5. Quick Start Example ğŸš€\n",
    "\n",
    "### ğŸ”° BEGINNER: Your First LLM Call\n",
    "\n",
    "Let's make our first call to an LLM using LangChain!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is LangChain in one sentence?\n",
      "\n",
      "Answer: LangChain is an open-source framework for building applications that leverage large language models. It provides modular componentsâ€”prompts, chains, agents, memory, and integrations with vector stores and toolsâ€”to orchestrate LLM calls and create end-to-end workflows.\n"
     ]
    }
   ],
   "source": [
    "#CELL-NO: 7\n",
    "# Import ChatOpenAI (the LLM interface)\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize the LLM\n",
    "# model: Which GPT model to use\n",
    "# temperature: 0 = deterministic, 1 = creative\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-5-nano\",  # Cheaper, faster model for learning\n",
    "    temperature=0  # Deterministic outputs for learning\n",
    ")\n",
    "\n",
    "# Make a simple call\n",
    "response = llm.invoke(\"What is LangChain in two sentences?\")\n",
    "\n",
    "# Print the response\n",
    "print(\"Question: What is LangChain in one sentence?\")\n",
    "print(f\"\\nAnswer: {response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FOR GEMINI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CELL-NO: 8\n",
    "!uv pip install -U langchain-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CELL-NO: 9\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is LangChain in one sentence?\n",
      "\n",
      "Answer: LangChain is a framework that simplifies the development of applications powered by large language models (LLMs) by providing tools to connect them with external data sources, computational resources, and other components in a structured way.\n"
     ]
    }
   ],
   "source": [
    "#CELL-NO: 10\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "model=\"gemini-2.5-flash\",  # Example Gemini model\n",
    "temperature=0  # Deterministic outputs for learning\n",
    ")\n",
    "\n",
    "# Make a simple call\n",
    "response = llm.invoke(\"What is LangChain in one sentence?\")\n",
    "\n",
    "# Print the response\n",
    "print(\"Question: What is LangChain in one sentence?\")\n",
    "print(f\"\\nAnswer: {response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ INTERMEDIATE: Understanding the Response Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response Type: <class 'langchain_core.messages.ai.AIMessage'>\n",
      "\n",
      "Content: LangChain is a framework that simplifies the development of applications powered by large language models (LLMs) by providing tools to connect them with external data sources, computational resources, and other components in a structured way.\n",
      "\n",
      "Response Metadata:\n",
      "{'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'model_provider': 'google_genai'}\n"
     ]
    }
   ],
   "source": [
    "#CELL-NO: 11\n",
    "# The response is an AIMessage object with metadata\n",
    "print(\"Response Type:\", type(response))\n",
    "print(\"\\nContent:\", response.content)\n",
    "print(\"\\nResponse Metadata:\")\n",
    "print(response.response_metadata)\n",
    "\n",
    "# Access specific metadata\n",
    "if 'token_usage' in response.response_metadata:\n",
    "    usage = response.response_metadata['token_usage']\n",
    "    print(f\"\\nTokens Used:\")\n",
    "    print(f\"  Prompt: {usage.get('prompt_tokens', 'N/A')}\")\n",
    "    print(f\"  Completion: {usage.get('completion_tokens', 'N/A')}\")\n",
    "    print(f\"  Total: {usage.get('total_tokens', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ”° BEGINNER: Using Prompts\n",
    "\n",
    "Instead of plain strings, use **prompt templates** for better control:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Template:\n",
      "Explain {topic} in simple terms suitable for beginners.\n"
     ]
    }
   ],
   "source": [
    "#CELL-NO: 12\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "# Create a prompt template\n",
    "# {topic} is a variable we'll fill in later\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Explain {topic} in simple terms suitable for beginners.\"\n",
    ")\n",
    "\n",
    "# View the prompt structure\n",
    "print(\"Prompt Template:\")\n",
    "print(prompt.messages[0].prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Topic: MACHINE LEARNING\n",
      "============================================================\n",
      "Imagine you want to teach a computer to do something, but you don't want to write *every single rule* for it. That's where **Machine Learning (ML)** comes in!\n",
      "\n",
      "Think of it like this:\n",
      "\n",
      "**It's a way of teaching computers to learn from data, much like how humans learn from experience, without being explicitly told every single step.**\n",
      "\n",
      "Here's a breakdown:\n",
      "\n",
      "1.  **Instead of Giving Rules, You Give Examples:**\n",
      "    *   If you wanted a computer to identify a cat in a photo, you *could* try to write rules like \"If it has pointed ears AND whiskers AND a tail AND certain eye shape...\" But that's incredibly hard because cats look different, lighting changes, etc.\n",
      "    *   With Machine Learning, you don't write these rules. Instead, you show the computer **thousands and thousands of pictures**, some labeled \"cat\" and some labeled \"not a cat.\"\n",
      "\n",
      "2.  **The Computer Finds Patterns:**\n",
      "    *   The computer then crunches all this data. It looks for common features, patterns, and relationships that consistently show up in the \"cat\" pictures but not in the \"not a cat\" pictures.\n",
      "    *   It essentially builds its *own internal \"rulebook\" or \"model\"* based on what it observes.\n",
      "\n",
      "3.  **It Gets Better with More Data (Experience):**\n",
      "    *   The more data (examples) you feed it, the smarter and more accurate it gets. If it makes a mistake (identifies a dog as a cat), you can tell it, and it will adjust its internal rulebook to avoid that mistake next time.\n",
      "\n",
      "4.  **It Can Then Make Predictions or Decisions on New Things:**\n",
      "    *   Once it has \"learned\" from the data, you can show it a *brand new picture* it has never seen before.\n",
      "    *   Using its learned patterns, it can then confidently say, \"Yep, that's a cat!\" (or \"No, that's not a cat!\").\n",
      "\n",
      "**Think of it like teaching a child:**\n",
      "\n",
      "You don't give a child a rulebook for \"what is a chair.\" You show them many different chairs (big, small, wooden, metal, office chairs, armchairs) and point to them saying \"chair.\" Eventually, they learn what a chair is and can identify a new one they've never seen before. That's essentially what machine learning does for computers.\n",
      "\n",
      "**Where do you see Machine Learning in action? Everywhere!**\n",
      "\n",
      "*   **Spam Filters:** Learning to identify patterns in spam emails to keep them out of your inbox.\n",
      "*   **Recommendation Systems:** Netflix recommending movies, Amazon suggesting products (\"People who liked X also liked Y\").\n",
      "*   **Voice Assistants:** Siri, Alexa, Google Assistant understanding your spoken commands.\n",
      "*   **Face Recognition:** Unlocking your phone with your face, or tagging friends in photos.\n",
      "*   **Self-Driving Cars:** Learning to identify other cars, pedestrians, traffic signs, and navigate.\n",
      "*   **Medical Diagnosis:** Helping doctors identify diseases from scans.\n",
      "\n",
      "In short, Machine Learning lets computers solve complex problems by learning from examples, adapting, and improving over time, rather than needing a human to write down every single instruction.\n",
      "\n",
      "============================================================\n",
      "Topic: EMBEDDINGS\n",
      "============================================================\n",
      "Imagine you want to teach a computer about things like words, sentences, pictures, or even sounds.\n",
      "\n",
      "**The Problem:**\n",
      "Computers don't understand meaning or concepts like humans do. For a computer, \"cat\" is just a string of letters, and a picture of a cat is just a grid of pixel values. It doesn't know what \"cat\" *means* or how it relates to \"kitten,\" \"dog,\" or \"purr.\"\n",
      "\n",
      "**The Solution: Embeddings!**\n",
      "\n",
      "An **embedding** is basically a way to turn something complex â€“ like a word, a sentence, an image, or even a whole document â€“ into a simple **list of numbers** (often called a vector).\n",
      "\n",
      "Think of it like giving everything a \"scorecard\" or a \"flavor profile\" for different attributes.\n",
      "\n",
      "**Analogy: Describing Food**\n",
      "\n",
      "Imagine you're trying to describe different foods using numbers:\n",
      "\n",
      "*   **Sweetness:** How sweet is it (0-10)?\n",
      "*   **Sourness:** How sour is it (0-10)?\n",
      "*   **Crunchiness:** How crunchy is it (0-10)?\n",
      "*   **Spiciness:** How spicy is it (0-10)?\n",
      "*   **Savoryness:** How savory is it (0-10)?\n",
      "\n",
      "Now, let's \"embed\" some foods:\n",
      "\n",
      "*   **Apple:** [Sweetness: 8, Sourness: 3, Crunchiness: 9, Spiciness: 0, Savoryness: 1]\n",
      "*   **Orange:** [Sweetness: 7, Sourness: 8, Crunchiness: 2, Spiciness: 0, Savoryness: 1]\n",
      "*   **Banana:** [Sweetness: 9, Sourness: 1, Crunchiness: 1, Spiciness: 0, Savoryness: 1]\n",
      "*   **Chili:** [Sweetness: 2, Sourness: 3, Crunchiness: 4, Spiciness: 9, Savoryness: 5]\n",
      "*   **Steak:** [Sweetness: 1, Sourness: 2, Crunchiness: 3, Spiciness: 2, Savoryness: 9]\n",
      "\n",
      "Each food is now represented by a list of 5 numbers. That list *is* its embedding.\n",
      "\n",
      "**The Magic of Embeddings:**\n",
      "\n",
      "The key thing about these numbers is that they are carefully generated so that:\n",
      "\n",
      "*   **Things with similar meanings or characteristics will have similar lists of numbers.**\n",
      "*   **Things with very different meanings will have very different lists of numbers.**\n",
      "\n",
      "Look at our food example:\n",
      "*   Apple, Orange, and Banana have pretty similar numbers for sweetness and very low numbers for spiciness. Their numerical lists are \"close\" to each other.\n",
      "*   Chili has a very high spiciness score, making its list of numbers quite different from the fruits.\n",
      "*   Steak has a high savoryness score, making it distinct from fruits and chili.\n",
      "\n",
      "You can imagine these lists of numbers placing each item in a vast, multi-dimensional \"space.\" Similar items end up close together in this space, and dissimilar items are far apart.\n",
      "\n",
      "**Why are Embeddings Powerful?**\n",
      "\n",
      "1.  **Computers can \"understand\" meaning:** Now, instead of just seeing \"cat\" and \"dog\" as different arbitrary letter sequences, the computer sees their numerical representations. It can then perform mathematical operations on these numbers.\n",
      "2.  **Finding Similarities:** If you want to find words similar to \"happy,\" the computer just looks for other words whose number lists are numerically \"close\" to \"happy's\" list. This is how:\n",
      "    *   **Recommendation Systems** (Netflix suggesting movies, Amazon suggesting products) work.\n",
      "    *   **Search Engines** understand your query even if you don't use the exact keywords.\n",
      "3.  **Understanding Relationships:** Embeddings can capture subtle relationships. For example, if you take the embedding for \"King,\" subtract the embedding for \"Man,\" and add the embedding for \"Woman,\" you often get an embedding that's very close to \"Queen\"! This shows it understands the gender and royalty relationship.\n",
      "4.  **Efficiency:** Instead of dealing with huge, complex data directly, the computer works with these much more compact and meaningful numerical representations.\n",
      "\n",
      "**In a Nutshell:**\n",
      "\n",
      "**Embeddings are a way to represent complex real-world items (like words, images, etc.) as lists of numbers, where those numbers capture the item's underlying meaning, characteristics, and relationships.** This allows computers to process information in a much smarter, more human-like way, enabling powerful applications like language understanding, image recognition, and personalized recommendations.\n",
      "\n",
      "============================================================\n",
      "Topic: VECTOR DATABASES\n",
      "============================================================\n",
      "Imagine you have a huge collection of things â€“ maybe songs, images, documents, or even ideas.\n",
      "\n",
      "Now, you want to find not just an *exact match* (like finding \"Bohemian Rhapsody\" in a list), but things that are *similar* in meaning, style, or content (like finding songs *similar* to \"Bohemian Rhapsody\"). This is where **vector databases** come in!\n",
      "\n",
      "Let's break it down with a simple analogy:\n",
      "\n",
      "---\n",
      "\n",
      "### The Analogy: Your Song Collection with \"Flavor Profiles\"\n",
      "\n",
      "Imagine you have thousands of songs. For each song, instead of just its title and artist, you create a \"flavor profile\" made of numbers.\n",
      "\n",
      "1.  **Turning Things into \"Flavor Profiles\" (Embeddings):**\n",
      "    *   You use a special program (like a chef's \"taste analyzer\") that listens to each song and assigns it a list of numbers based on its characteristics:\n",
      "        *   **Song 1 (Bohemian Rhapsody):** [8.2 (rock), 7.5 (dramatic), 6.1 (complex), 3.4 (opera-like), ...]\n",
      "        *   **Song 2 (Stairway to Heaven):** [9.0 (rock), 7.0 (dramatic), 6.5 (complex), 2.1 (folk-rock), ...]\n",
      "        *   **Song 3 (Happy Birthday):** [1.1 (pop), 0.5 (simple), 0.2 (children's), 0.1 (acoustic), ...]\n",
      "    *   This list of numbers is called a **vector**. It's like a coordinate point in a vast \"taste space.\" Songs that are musically similar will have vectors that are \"close\" to each other in this space. This process of turning complex data (like a song) into a numerical vector is called **embedding**.\n",
      "\n",
      "2.  **Storing Your \"Flavor Profiles\" (The Vector Database):**\n",
      "    *   A **vector database** is a special kind of database designed to store these numerical \"flavor profiles\" (vectors) very efficiently.\n",
      "    *   Unlike a regular database that might store text or simple numbers for exact lookup, a vector database is optimized for finding *similarity*.\n",
      "\n",
      "3.  **Finding Similar Songs (Similarity Search):**\n",
      "    *   Now, you play a new song and ask the database: \"Find me songs that are *similar* to this one!\"\n",
      "    *   First, your new song also gets its \"flavor profile\" (vector) generated.\n",
      "    *   The vector database then quickly looks through all the stored song vectors and finds the ones whose \"flavor profiles\" (vectors) are numerically closest to your new song's vector.\n",
      "    *   It doesn't just match keywords; it understands the *essence* or *meaning* of the song based on its numerical representation.\n",
      "\n",
      "---\n",
      "\n",
      "### In Simple Terms:\n",
      "\n",
      "*   **You have complex data:** Images, text, audio, videos, ideas.\n",
      "*   **You convert it into a list of numbers (a \"vector\"):** Think of this as a numerical \"fingerprint\" or \"flavor profile\" that captures its core characteristics and meaning. This process is called **embedding**, often done by AI.\n",
      "*   **A Vector Database stores these numerical fingerprints:** It's specifically built to handle and search these lists of numbers super fast.\n",
      "*   **Its superpower is \"similarity search\":** You give it a new numerical fingerprint, and it quickly finds all the other fingerprints in its collection that are \"close enough\" or \"most similar\" to it.\n",
      "\n",
      "### Why are they important now?\n",
      "\n",
      "Vector databases are crucial for modern AI applications, especially with the rise of Large Language Models (LLMs) like ChatGPT:\n",
      "\n",
      "*   **Semantic Search:** Instead of searching for exact keywords, you can search for the *meaning* of a query. \"Show me pictures of happy dogs\" will find actual happy dogs, not just images with the words \"happy\" and \"dogs\" in their filenames.\n",
      "*   **Recommendations:** \"People who liked this song/movie/product also liked these...\" is powered by finding similar item vectors.\n",
      "*   **Generative AI (like ChatGPT):** Vector databases help these AIs quickly retrieve relevant information or context to answer questions more accurately and keep their responses consistent.\n",
      "*   **Image Recognition:** Finding visually similar images.\n",
      "*   **Anomaly Detection:** Finding things that are *not* similar to anything else (e.g., fraudulent transactions).\n",
      "\n",
      "So, a **vector database** is essentially a specialized system for storing and rapidly finding *similar* items based on their numerical \"flavor profiles\" or \"fingerprints.\"\n"
     ]
    }
   ],
   "source": [
    "#CELL-NO: 13\n",
    "# Build a chain: Prompt â†’ LLM\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Components:\n",
    "# 1. prompt: Formats the input\n",
    "# 2. llm: Generates the response\n",
    "# 3. StrOutputParser: Extracts just the text from the response\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Use the chain with different topics\n",
    "topics = [\"machine learning\", \"embeddings\", \"vector databases\"]\n",
    "\n",
    "for topic in topics:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Topic: {topic.upper()}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    # Invoke the chain\n",
    "    explanation = chain.invoke({\"topic\": topic})\n",
    "    print(explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ INTERMEDIATE: Batch Processing\n",
    "\n",
    "Process multiple inputs efficiently using `.batch()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CELL-NO: 14\n",
    "# Batch process multiple topics at once\n",
    "topics_batch = [\n",
    "    {\"topic\": \"RAG\"},\n",
    "    {\"topic\": \"LCEL\"},\n",
    "    {\"topic\": \"LangChain agents\"}\n",
    "]\n",
    "\n",
    "# Batch processing is more efficient than calling invoke() multiple times\n",
    "results = chain.batch(topics_batch)\n",
    "\n",
    "# Print results\n",
    "for i, (input_dict, result) in enumerate(zip(topics_batch, results), 1):\n",
    "    print(f\"\\n{i}. {input_dict['topic'].upper()}:\")\n",
    "    print(f\"   {result[:100]}...\")  # Print first 100 chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"comparison\"></a>\n",
    "## 6. LangChain vs Traditional ML Pipelines ğŸ†š\n",
    "\n",
    "### ğŸ”° BEGINNER: Key Differences\n",
    "\n",
    "| Aspect | Traditional ML | LangChain |\n",
    "|--------|---------------|------------|\n",
    "| **Setup** | Complex, manual | Pre-built components |\n",
    "| **Integration** | Write custom code | Use existing integrations |\n",
    "| **Composability** | Difficult to chain | Easy with LCEL |\n",
    "| **Debugging** | Manual logging | Built-in callbacks |\n",
    "| **Prototyping** | Slow | Very fast |\n",
    "| **Code Reuse** | Limited | High |\n",
    "\n",
    "### Example: Building a Q&A System\n",
    "\n",
    "#### Without LangChain (50+ lines):\n",
    "```python\n",
    "import openai\n",
    "import PyPDF2\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load PDF manually\n",
    "def load_pdf(file_path):\n",
    "    reader = PyPDF2.PdfReader(file_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "# 2. Split text manually\n",
    "def split_text(text, chunk_size=1000):\n",
    "    chunks = []\n",
    "    for i in range(0, len(text), chunk_size):\n",
    "        chunks.append(text[i:i+chunk_size])\n",
    "    return chunks\n",
    "\n",
    "# 3. Create embeddings manually\n",
    "def create_embeddings(chunks):\n",
    "    embeddings = []\n",
    "    for chunk in chunks:\n",
    "        response = openai.Embedding.create(\n",
    "            input=chunk,\n",
    "            model=\"text-embedding-ada-002\"\n",
    "        )\n",
    "        embeddings.append(response['data'][0]['embedding'])\n",
    "    return np.array(embeddings)\n",
    "\n",
    "# 4. Create vector store manually\n",
    "def create_vector_store(embeddings):\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(embeddings)\n",
    "    return index\n",
    "\n",
    "# 5. Search manually\n",
    "def search(query, index, chunks):\n",
    "    query_embedding = openai.Embedding.create(\n",
    "        input=query,\n",
    "        model=\"text-embedding-ada-002\"\n",
    "    )['data'][0]['embedding']\n",
    "    \n",
    "    distances, indices = index.search(\n",
    "        np.array([query_embedding]), k=3\n",
    "    )\n",
    "    return [chunks[i] for i in indices[0]]\n",
    "\n",
    "# 6. Generate answer manually\n",
    "def generate_answer(query, context):\n",
    "    prompt = f\"Context: {context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "# Use it:\n",
    "text = load_pdf(\"document.pdf\")\n",
    "chunks = split_text(text)\n",
    "embeddings = create_embeddings(chunks)\n",
    "index = create_vector_store(embeddings)\n",
    "relevant_chunks = search(\"What is RAG?\", index, chunks)\n",
    "answer = generate_answer(\"What is RAG?\", \" \".join(relevant_chunks))\n",
    "```\n",
    "\n",
    "#### With LangChain (10 lines!):\n",
    "```python\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Load, split, embed, and store\n",
    "docs = PyPDFLoader(\"document.pdf\").load()\n",
    "chunks = RecursiveCharacterTextSplitter(chunk_size=1000).split_documents(docs)\n",
    "vectorstore = FAISS.from_documents(chunks, OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Build RAG chain\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Context: {context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    ")\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt | ChatOpenAI() | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Use it:\n",
    "answer = chain.invoke(\"What is RAG?\")\n",
    "```\n",
    "\n",
    "**50+ lines â†’ 10 lines!** ğŸ‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"summary\"></a>\n",
    "## 7. Summary & Next Steps ğŸ“\n",
    "\n",
    "### ğŸ‰ What You Learned\n",
    "\n",
    "âœ… **LangChain** is a framework that simplifies building LLM applications\n",
    "\n",
    "âœ… **Architecture** is modular: loaders, splitters, embeddings, vector stores, chains\n",
    "\n",
    "âœ… **Documents** contain `page_content` (text) and `metadata` (information about the text)\n",
    "\n",
    "âœ… **LCEL** uses the `|` operator to chain components together\n",
    "\n",
    "âœ… **Chains** connect multiple components to create complex workflows\n",
    "\n",
    "âœ… LangChain **dramatically reduces code** compared to manual implementations\n",
    "\n",
    "### ğŸ”° For Beginners\n",
    "You now understand:\n",
    "- What LangChain is and why it's useful\n",
    "- How to set up your environment\n",
    "- Basic concepts: Documents and Chains\n",
    "- How to make your first LLM call\n",
    "\n",
    "### ğŸ“ For Intermediate Learners\n",
    "You now understand:\n",
    "- LangChain's package structure (1.0+ reorganization)\n",
    "- LCEL internals and advantages\n",
    "- Metadata usage for filtering and citation\n",
    "- Batch processing for efficiency\n",
    "\n",
    "### ğŸ“š Next Notebooks\n",
    "\n",
    "1. **Notebook 02**: Document Loaders (PDF, CSV, JSON, HTML)\n",
    "2. **Notebook 03**: Text Splitting Strategies\n",
    "3. **Notebook 04**: Embeddings and Vector Representations\n",
    "4. **Notebook 05**: Vector Stores (FAISS, Chroma)\n",
    "5. **Notebook 06**: Retrieval Strategies\n",
    "6. **Notebook 07**: Complete RAG Pipeline\n",
    "\n",
    "### ğŸ’¡ Practice Exercises\n",
    "\n",
    "Before moving to the next notebook, try these:\n",
    "\n",
    "1. **Easy**: Create 5 Documents about different topics with meaningful metadata\n",
    "2. **Medium**: Build a chain that takes a topic and generates a haiku about it\n",
    "3. **Advanced**: Create a chain that summarizes text in different styles (formal, casual, technical)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ Additional Resources\n",
    "\n",
    "- [Official LangChain Documentation](https://python.langchain.com/docs/)\n",
    "- [LCEL Guide](https://python.langchain.com/docs/expression_language/)\n",
    "- [LangChain GitHub](https://github.com/langchain-ai/langchain)\n",
    "\n",
    "---\n",
    "\n",
    "**Ready for more? Continue to Notebook 02: Document Loaders! ğŸš€**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
